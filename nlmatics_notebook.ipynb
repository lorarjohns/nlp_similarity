{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outline of Strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For complex natural language understanding tasks like detecting paraphrases, duplicates, or plagiarism, the state of the art is complex neural networks using transformers and attention, usually with the pretrained resources released by Google, HuggingFace, and those few who have the enormous resources it requires to compute these language models. Even with these resources, deep NLU is a nontrivial task. For the best performance, they must be fine-tuned with compatible, well-prepared custom data; the correct models and hyperparameters for the use case must be carefully selected; and the interpretability of the model must be seriously considered.\n",
    "\n",
    "I would approach this problem, depending on the urgency of the issue and the immediate, medium-term, and long-term needs of the client/business, as follows: \n",
    "\n",
    "- Perform a preliminary analysis of the data in spaCy, into which we can load out-of-the-box BERT and relatively quickly model our data and sketch out several different analyses to get an idea of how best to proceed with fine-tuning. SpaCy also provides the advantages of excellent visualization interfaces and relatively high interpretability in this context of deep learning.\n",
    "- Migrate our model to PyTorch, where it is possible to fine-tune both the data and the model in a much more granular way. We may find it advantageous to experiment with Siamese LSTMs, etc., and to take advantage of the many well-written and maintained PyTorch libraries from AllenNLP and others. \n",
    "- If speed and interpretability are of paramount importance, then I would implement doc2vec/sent2vec in gensim, which has low computational complexity and is fast and parallelizable, yet has very good downstream performance on a variety of tasks. Furthermore, using t-Distributed Stochastic Neighbor Embedding, we can produce visualizations (in Tensorboard or otherwise) of which words and phrases, precisely, are most similar to each other, and thus contribute most to the result.\n",
    "\n",
    "Some additional important considerations: \n",
    "\n",
    "- Customization will be important if we seek to find synonyms in any kind of specialized domain, like law or finance, where the usual distance measures may produce less than satisfactory results without the ability to add more training data efficiently. We may also be able to take advantage of hierarchical training, e.g., training image recognition simultaneously for semantic enrichment.\n",
    "- SpaCy only saves the final hidden state of the model, so we're unable to inquire about its \"thought process\" at arriving there. In future versions, this feature will be available, but it's a good reason to use PyTorch --especially since visualization tools for recovering and interpreting the weights of hidden layers' states are becoming more common and more revealing.\n",
    "- I have not yet implemented dependency parsing, part-of-speech tagging, or many other features that can be structurally important, partially because the scope of this notebook will soon become very expansive, and partially because BERT accounts for a surprising amount of the variance. We could show spaCy dependency parsing, e.g. visually with displaCy or as a feature, but it would require a parallel-trained model, as BERT-trained models cannot also retain dependency/NER/etc. in spaCy. That's a limitation of this approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SpaCy using BERT transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-25T17:45:38.788007Z",
     "start_time": "2019-11-25T17:45:34.945046Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "import torch\n",
    "import spacy\n",
    "from numpy.testing import assert_almost_equal\n",
    "\n",
    "import pandas as pd\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-25T17:45:49.078354Z",
     "start_time": "2019-11-25T17:45:38.816834Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1125 17:45:40.600836 140283058251648 file_utils.py:39] PyTorch version 1.2.0 available.\n",
      "I1125 17:45:41.085140 140283058251648 modeling_xlnet.py:194] Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .\n",
      "I1125 17:45:41.272373 140283058251648 modeling_bert.py:226] Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .\n",
      "I1125 17:45:41.293509 140283058251648 modeling_xlnet.py:339] Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_trf_bertbaseuncased_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-25T17:45:49.131152Z",
     "start_time": "2019-11-25T17:45:49.093031Z"
    },
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 102: expected 5 fields, saw 6\\nSkipping line 656: expected 5 fields, saw 6\\nSkipping line 867: expected 5 fields, saw 6\\nSkipping line 880: expected 5 fields, saw 6\\nSkipping line 980: expected 5 fields, saw 6\\nSkipping line 1439: expected 5 fields, saw 6\\nSkipping line 1473: expected 5 fields, saw 6\\nSkipping line 1822: expected 5 fields, saw 6\\nSkipping line 1952: expected 5 fields, saw 6\\nSkipping line 2009: expected 5 fields, saw 6\\nSkipping line 2230: expected 5 fields, saw 6\\nSkipping line 2506: expected 5 fields, saw 6\\nSkipping line 2523: expected 5 fields, saw 6\\nSkipping line 2809: expected 5 fields, saw 6\\nSkipping line 2887: expected 5 fields, saw 6\\nSkipping line 2920: expected 5 fields, saw 6\\nSkipping line 2944: expected 5 fields, saw 6\\nSkipping line 3241: expected 5 fields, saw 6\\nSkipping line 3358: expected 5 fields, saw 6\\nSkipping line 3459: expected 5 fields, saw 6\\n'\n"
     ]
    }
   ],
   "source": [
    "text = pd.read_csv(r\"msr-para-train.tsv\", \n",
    "                   sep=\"\\t\", header=0, error_bad_lines=False)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-25T17:53:06.719035Z",
     "start_time": "2019-11-25T17:53:06.709429Z"
    }
   },
   "outputs": [],
   "source": [
    "# Duplicate labels so it's easier to process the data\n",
    "text[\"label_1\"] = text[\"Quality\"]\n",
    "text[\"label_2\"] = text[\"Quality\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-25T17:53:08.815918Z",
     "start_time": "2019-11-25T17:53:08.809887Z"
    }
   },
   "outputs": [],
   "source": [
    "# Rename the columns\n",
    "text.columns = [\"label\", \"id_1\", \"id_2\", \"doc_1\", \n",
    "                \"doc_2\", \"label_1\", \"label_2\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-25T17:53:11.401404Z",
     "start_time": "2019-11-25T17:53:11.385234Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>id_1</th>\n",
       "      <th>id_2</th>\n",
       "      <th>doc_1</th>\n",
       "      <th>doc_2</th>\n",
       "      <th>label_1</th>\n",
       "      <th>label_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>702876</td>\n",
       "      <td>702977</td>\n",
       "      <td>Amrozi accused his brother, whom he called \"th...</td>\n",
       "      <td>Referring to him as only \"the witness\", Amrozi...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2108705</td>\n",
       "      <td>2108831</td>\n",
       "      <td>Yucaipa owned Dominick's before selling the ch...</td>\n",
       "      <td>Yucaipa bought Dominick's in 1995 for $693 mil...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1330381</td>\n",
       "      <td>1330521</td>\n",
       "      <td>They had published an advertisement on the Int...</td>\n",
       "      <td>On June 10, the ship's owners had published an...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3344667</td>\n",
       "      <td>3344648</td>\n",
       "      <td>Around 0335 GMT, Tab shares were up 19 cents, ...</td>\n",
       "      <td>Tab shares jumped 20 cents, or 4.6%, to set a ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1236820</td>\n",
       "      <td>1236712</td>\n",
       "      <td>The stock rose $2.11, or about 11 percent, to ...</td>\n",
       "      <td>PG&amp;E Corp. shares jumped $1.63 or 8 percent to...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label     id_1     id_2                                              doc_1  \\\n",
       "0      1   702876   702977  Amrozi accused his brother, whom he called \"th...   \n",
       "1      0  2108705  2108831  Yucaipa owned Dominick's before selling the ch...   \n",
       "2      1  1330381  1330521  They had published an advertisement on the Int...   \n",
       "3      0  3344667  3344648  Around 0335 GMT, Tab shares were up 19 cents, ...   \n",
       "4      1  1236820  1236712  The stock rose $2.11, or about 11 percent, to ...   \n",
       "\n",
       "                                               doc_2  label_1  label_2  \n",
       "0  Referring to him as only \"the witness\", Amrozi...        1        1  \n",
       "1  Yucaipa bought Dominick's in 1995 for $693 mil...        0        0  \n",
       "2  On June 10, the ship's owners had published an...        1        1  \n",
       "3  Tab shares jumped 20 cents, or 4.6%, to set a ...        0        0  \n",
       "4  PG&E Corp. shares jumped $1.63 or 8 percent to...        1        1  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-25T17:55:56.581485Z",
     "start_time": "2019-11-25T17:55:56.576540Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'apple'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"Apple\".lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-25T19:05:51.819725Z",
     "start_time": "2019-11-25T19:05:51.810403Z"
    }
   },
   "outputs": [],
   "source": [
    "# lowercase the documents, because our BERT model is the uncased version\n",
    "text[\"doc_1_proc\"] = text[\"doc_1\"].apply(str.lower)\n",
    "text[\"doc_2_proc\"] = text[\"doc_2\"].apply(str).apply(str.lower)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more fine-tuning and extensibility of a model, eventually it would be a good idea to use a framework like PyTorch, with, e.g., the HuggingFace transformers, which can be trained online, and which supports any kind of custom model you might want to write. Some state-of-the-art methods for resolving the paraphrase question include Siamese recurrent architectures (e.g. LSTMs) and other parallel models, which can be implemented in PyTorch.\n",
    "\n",
    "However, these can be slow to train and require a lot of time to perfect. For a fast way to get a reasonable idea of the data, spaCy does a remarkably good job, especially with its integration with the PyTorch BERT model. Furthermore, it contains a mapping from the BERT word part embeddings back to the spaCy token ids, meaning that it is more immediately interpretable than some other models, which would require a lot of convoluted reasoning -- pun intended -- to interpret the weights of the hidden layers, and even then, the meaning may not be entirely clear. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-25T17:58:38.490748Z",
     "start_time": "2019-11-25T17:58:35.304437Z"
    }
   },
   "outputs": [],
   "source": [
    "# We load the uncased BERT model, so we will need to lowercase our corpus.\n",
    "nlp = spacy.load(\"en_trf_bertbaseuncased_lg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERT's WordPiece model checks whether the whole word is in the vocabulary. If not, it recursively decomposes it into subwords, and finally characters. This greedy algorithm means that, when it sees an out-of-vocabulary word, the model can _always_ represent it, if only as the average of its characters' vectors. BERT thus preserves more context than models that use explicit tags and assign unknown tokens to a catch-all category. \n",
    "\n",
    "The transformer model BERT was trained on uses self-attention, with a mask in the decoder, but not the encoder, and also includes a next-sentence prediction task that shuffles the corpus. It derives most of its power from the fact that it learns from itself: at a high level, tokens derive their embeddings from matrix multiplication (a measure of distance or similarity), weighted by the embedding vector as context. Every word in the corpus has multiple, bi-directional, context-dependent embeddings, allowing for more nuanced queries and understanding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-25T18:01:09.865010Z",
     "start_time": "2019-11-25T18:01:09.736617Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amrozi accused his brother, whom he called \"the witness\", of deliberately distorting his evidence.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(19, 768)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(text[\"doc_1\"].tolist()[0])\n",
    "print(doc)\n",
    "doc.tensor.shape # (7, 768)  # Always has one row per token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that BERT has mapped the OOV token \"Amrozi\" to several word parts (\"Am-ro-zi\"), and the word \"distorting\" into pieces that map roughly to morphemes or phonological syllabic units (\"di-stor-ting\"). We can also access the alignment via the `doc._.trf_alignment` attribute. `[CLS]` and `[SEP]` denote the beginning and end of a document and are important tokens in their own right; note that they, too, receive attention scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-25T18:22:42.750286Z",
     "start_time": "2019-11-25T18:22:42.696366Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wordpieces and attention scores for sample 1:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>wordpiece_id</th>\n",
       "      <th>wordpiece_text</th>\n",
       "      <th>last_hidden_state</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>101</td>\n",
       "      <td>[CLS]</td>\n",
       "      <td>[-0.5703974, 0.32815552, -0.5710026, -0.379076...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2572</td>\n",
       "      <td>am</td>\n",
       "      <td>[-0.12826154, 0.013061862, -0.60954684, -1.305...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3217</td>\n",
       "      <td>##ro</td>\n",
       "      <td>[0.6386509, -0.3578455, -0.054456137, -1.19054...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>5831</td>\n",
       "      <td>##zi</td>\n",
       "      <td>[0.43038714, 0.1304761, -0.43265325, -0.924443...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5496</td>\n",
       "      <td>accused</td>\n",
       "      <td>[0.1592648, 0.06319537, -0.12583436, -0.546904...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2010</td>\n",
       "      <td>his</td>\n",
       "      <td>[0.2193718, -0.12850477, -0.5328888, -0.092491...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>2567</td>\n",
       "      <td>brother</td>\n",
       "      <td>[-0.008652532, -0.023832224, -0.8758689, -0.24...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1010</td>\n",
       "      <td>,</td>\n",
       "      <td>[-0.54377395, 0.25359523, 0.027289549, -0.0495...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>3183</td>\n",
       "      <td>whom</td>\n",
       "      <td>[-0.5229964, 0.4036232, 0.3030943, -0.41309872...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>2002</td>\n",
       "      <td>he</td>\n",
       "      <td>[-0.17844011, 0.21058142, -0.4347112, -0.10529...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2170</td>\n",
       "      <td>called</td>\n",
       "      <td>[-0.026414776, 0.1807914, -0.12721822, -0.4814...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>1000</td>\n",
       "      <td>\"</td>\n",
       "      <td>[-0.9571262, 0.04264796, -0.27522767, -0.72554...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>1996</td>\n",
       "      <td>the</td>\n",
       "      <td>[-0.11569587, -0.23793547, 0.187001, 0.3656879...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>7409</td>\n",
       "      <td>witness</td>\n",
       "      <td>[-0.3199614, 0.22462964, 0.44129184, -0.046602...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>1000</td>\n",
       "      <td>\"</td>\n",
       "      <td>[-0.3683752, 0.7087346, -0.13061877, -0.996288...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>1010</td>\n",
       "      <td>,</td>\n",
       "      <td>[0.31144065, 0.10733642, 0.074744396, 0.306048...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>1997</td>\n",
       "      <td>of</td>\n",
       "      <td>[-0.9027344, -0.2411271, -0.2994182, -1.436053...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>9969</td>\n",
       "      <td>deliberately</td>\n",
       "      <td>[0.61915743, 0.08203058, -0.16542655, -0.61755...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>4487</td>\n",
       "      <td>di</td>\n",
       "      <td>[0.24152946, 0.029961728, -0.38713264, -0.2262...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>23809</td>\n",
       "      <td>##stor</td>\n",
       "      <td>[0.82673454, 0.27901563, -0.2961575, -0.172363...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>3436</td>\n",
       "      <td>##ting</td>\n",
       "      <td>[-0.03780017, -0.14117621, -0.37119934, 0.0173...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>2010</td>\n",
       "      <td>his</td>\n",
       "      <td>[0.042272024, 0.1276438, -0.494527, 0.15906604...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>3350</td>\n",
       "      <td>evidence</td>\n",
       "      <td>[0.13280264, 0.24047671, -0.4944409, -0.087198...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>1012</td>\n",
       "      <td>.</td>\n",
       "      <td>[0.3688815, -0.029209737, -0.1438845, 0.306532...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>102</td>\n",
       "      <td>[SEP]</td>\n",
       "      <td>[-0.21118157, 0.0980026, -0.27321264, -0.98750...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    wordpiece_id wordpiece_text  \\\n",
       "0            101          [CLS]   \n",
       "1           2572             am   \n",
       "2           3217           ##ro   \n",
       "3           5831           ##zi   \n",
       "4           5496        accused   \n",
       "5           2010            his   \n",
       "6           2567        brother   \n",
       "7           1010              ,   \n",
       "8           3183           whom   \n",
       "9           2002             he   \n",
       "10          2170         called   \n",
       "11          1000              \"   \n",
       "12          1996            the   \n",
       "13          7409        witness   \n",
       "14          1000              \"   \n",
       "15          1010              ,   \n",
       "16          1997             of   \n",
       "17          9969   deliberately   \n",
       "18          4487             di   \n",
       "19         23809         ##stor   \n",
       "20          3436         ##ting   \n",
       "21          2010            his   \n",
       "22          3350       evidence   \n",
       "23          1012              .   \n",
       "24           102          [SEP]   \n",
       "\n",
       "                                    last_hidden_state  \n",
       "0   [-0.5703974, 0.32815552, -0.5710026, -0.379076...  \n",
       "1   [-0.12826154, 0.013061862, -0.60954684, -1.305...  \n",
       "2   [0.6386509, -0.3578455, -0.054456137, -1.19054...  \n",
       "3   [0.43038714, 0.1304761, -0.43265325, -0.924443...  \n",
       "4   [0.1592648, 0.06319537, -0.12583436, -0.546904...  \n",
       "5   [0.2193718, -0.12850477, -0.5328888, -0.092491...  \n",
       "6   [-0.008652532, -0.023832224, -0.8758689, -0.24...  \n",
       "7   [-0.54377395, 0.25359523, 0.027289549, -0.0495...  \n",
       "8   [-0.5229964, 0.4036232, 0.3030943, -0.41309872...  \n",
       "9   [-0.17844011, 0.21058142, -0.4347112, -0.10529...  \n",
       "10  [-0.026414776, 0.1807914, -0.12721822, -0.4814...  \n",
       "11  [-0.9571262, 0.04264796, -0.27522767, -0.72554...  \n",
       "12  [-0.11569587, -0.23793547, 0.187001, 0.3656879...  \n",
       "13  [-0.3199614, 0.22462964, 0.44129184, -0.046602...  \n",
       "14  [-0.3683752, 0.7087346, -0.13061877, -0.996288...  \n",
       "15  [0.31144065, 0.10733642, 0.074744396, 0.306048...  \n",
       "16  [-0.9027344, -0.2411271, -0.2994182, -1.436053...  \n",
       "17  [0.61915743, 0.08203058, -0.16542655, -0.61755...  \n",
       "18  [0.24152946, 0.029961728, -0.38713264, -0.2262...  \n",
       "19  [0.82673454, 0.27901563, -0.2961575, -0.172363...  \n",
       "20  [-0.03780017, -0.14117621, -0.37119934, 0.0173...  \n",
       "21  [0.042272024, 0.1276438, -0.494527, 0.15906604...  \n",
       "22  [0.13280264, 0.24047671, -0.4944409, -0.087198...  \n",
       "23  [0.3688815, -0.029209737, -0.1438845, 0.306532...  \n",
       "24  [-0.21118157, 0.0980026, -0.27321264, -0.98750...  "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Wordpieces and attention scores for sample 1:\")\n",
    "alignment_table = pd.DataFrame(list(zip(doc._.trf_word_pieces, doc._.trf_word_pieces_, doc._.trf_last_hidden_state)), \n",
    "             columns=[\"wordpiece_id\", \"wordpiece_text\", \"last_hidden_state\"])\n",
    "alignment_table.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-25T02:50:47.755631Z",
     "start_time": "2019-11-25T02:50:47.747920Z"
    }
   },
   "outputs": [],
   "source": [
    "# match the sum-pooled vectors as nearly as possible\n",
    "assert_almost_equal(doc.tensor.sum(axis=0), doc._.trf_last_hidden_state.sum(axis=0), decimal=5)\n",
    "span = doc[2:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can quickly obtain the cosine distance between any two docs or doc subparts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-25T18:31:52.130299Z",
     "start_time": "2019-11-25T18:31:51.899900Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The similarity score of known synonyms: 0.9723842587615814\n",
      "The similarity score of unrelated samples: 0.6042276210130809\n"
     ]
    }
   ],
   "source": [
    "doc_b = nlp(text[\"doc_2\"].tolist()[0]) # This is the known paraphrase of our doc\n",
    "print(f\"The similarity score of known synonyms: {doc.similarity(doc_b)}\")\n",
    "doc2 = nlp(text[\"doc_1\"].tolist()[1]) # This is an unrelated sample\n",
    "print(f\"The similarity score of unrelated samples: {doc.similarity(doc2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SpaCy allows us to acces the values of any span tensor we desire. This is meaningfully possible because of the way BERT tokenizes and encodes a corpus. It gives us the power to directly compare sentences or subparts of sentences using various distance methods, based on their vectors, very quickly, and to see whether those methods accord with intuition. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-25T02:53:35.091537Z",
     "start_time": "2019-11-25T02:53:35.081023Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Here is some text to encode."
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Access the tensor from Span elements (especially helpful for sentences)\n",
    "assert numpy.array_equal(span.tensor, doc.tensor[2:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can input any text we like and see the similarity scores for:\n",
    "\n",
    "- the entire input\n",
    "- any token in the input\n",
    "- any span in the input\n",
    "\n",
    "Therefore, we can adjust granularity with which we examine which components contribute to the _overall_ similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-25T21:01:14.727815Z",
     "start_time": "2019-11-25T21:01:14.441547Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8991707\n",
      "0.658642\n"
     ]
    }
   ],
   "source": [
    "bert1 = nlp(\"BERT is a technologically ground-breaking natural language processing model\")\n",
    "bert2 = nlp(\"BERT is referred to as a model in many articles, however, it is more of a framework\")\n",
    "bert3 = nlp(\"Bert and Ernie were built by Don Sahlin from a simple design scribbled by Jim Henson|\")\n",
    "print(bert1[0].similarity(bert2[0]))  \n",
    "print(bert1[0].similarity(bert3[0]))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-25T21:01:16.319781Z",
     "start_time": "2019-11-25T21:01:16.290269Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token1</th>\n",
       "      <th>token2</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>BERT</td>\n",
       "      <td>BERT</td>\n",
       "      <td>0.899171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>208</td>\n",
       "      <td>model</td>\n",
       "      <td>framework</td>\n",
       "      <td>0.737894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>is</td>\n",
       "      <td>is</td>\n",
       "      <td>0.684145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>0.646669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>a</td>\n",
       "      <td>is</td>\n",
       "      <td>0.621150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>BERT</td>\n",
       "      <td>,</td>\n",
       "      <td>0.088770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>BERT</td>\n",
       "      <td>is</td>\n",
       "      <td>0.084505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>BERT</td>\n",
       "      <td>to</td>\n",
       "      <td>0.081156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>114</td>\n",
       "      <td>breaking</td>\n",
       "      <td>BERT</td>\n",
       "      <td>0.065998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>BERT</td>\n",
       "      <td>,</td>\n",
       "      <td>0.004659</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>209 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       token1     token2     score\n",
       "0        BERT       BERT  0.899171\n",
       "208     model  framework  0.737894\n",
       "20         is         is  0.684145\n",
       "43          a          a  0.646669\n",
       "39          a         is  0.621150\n",
       "..        ...        ...       ...\n",
       "10       BERT          ,  0.088770\n",
       "14       BERT         is  0.084505\n",
       "3        BERT         to  0.081156\n",
       "114  breaking       BERT  0.065998\n",
       "12       BERT          ,  0.004659\n",
       "\n",
       "[209 rows x 3 columns]"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get token-by-token similarity scores for the sentences above\n",
    "\n",
    "sims = []\n",
    "for token in bert1:\n",
    "    for compare in bert2:\n",
    "        sims.append((token.text, compare.text, token.similarity(compare)))\n",
    "sims_df = pd.DataFrame(sims, columns=[\"token1\", \"token2\", \"score\"])\n",
    "\n",
    "# Sort the similarity scores in descending order\n",
    "\n",
    "sims_df.sort_values(by=[\"score\"], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-25T21:04:14.139329Z",
     "start_time": "2019-11-25T21:04:14.133926Z"
    }
   },
   "outputs": [],
   "source": [
    "# Spans have tensors as well\n",
    "span = bert3[2:4]\n",
    "assert numpy.array_equal(span.tensor, bert3.tensor[2:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gensim doc2vec with earth mover's distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If interpretability is a primary concern, then neural networks of the complexity that BERT and associated models introduce may not be the best choice, because it's much harder to recover the \"reasoning\" for the output. Moreover, as more models are pruned intentionally for \"selective brain damage\" and information is lost, it's been shown that hard-to-predict classes are disproportionately affected. This isn't always desirable. Additionally, it may not be necessary to use the expensive, computationally intense tools to do a job that traditional ones can handle just fine.\n",
    "\n",
    "Implementations of word2vec-like shallow neural networks dramatically improve on the token-level bag-of-words model. They're more context aware because the underlying model is trained to calculate probabilities on sequences of tokens within a given window. Thus, the model retains more structural information about terms as they relate to each other in context. Sense2vec, sent2vec, and doc2vec can produce satisfactorily context-aware representations of larger subparts of a corpus for many purposes, with the benefit of being more explainable than their convolutional, and convoluted, neural counterparts.\n",
    "\n",
    "Techniques like singular value decomposition (latent semantic indexing/analysis) have a sound mathematical basis for dimensionality reduction, and make intuitive sense with respect to covariance explainability.\n",
    "\n",
    "We can ameliorate some of the word2vec family of methods' context issues to a degree by adding ngram models. Really, this echoes BERT's strategy of iteratively breaking down and encoding smaller subparts of a language to \"overfit\" it as closely as possible, except less extensively and precisely. However, we gain speed and economy, flexibility, and interpretability in the bargain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-25T19:54:17.538006Z",
     "start_time": "2019-11-25T19:54:17.531794Z"
    }
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim import corpora\n",
    "from gensim.models import Phrases\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-25T22:52:30.517634Z",
     "start_time": "2019-11-25T22:52:30.430547Z"
    }
   },
   "outputs": [],
   "source": [
    "# to do: refit to glove model (still downloading!)\n",
    "glove_model = gensim.models.KeyedVectors.load_word2vec_format('../model/text/stanford/glove/glove.6B.50d.vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-25T19:10:32.554129Z",
     "start_time": "2019-11-25T19:05:58.715688Z"
    }
   },
   "outputs": [],
   "source": [
    "text[\"doc_1_proc\"] = list(nlp.pipe(text[\"doc_1_proc\"].tolist()))\n",
    "text[\"doc_2_proc\"] = list(nlp.pipe(text[\"doc_2_proc\"].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-25T19:18:26.993879Z",
     "start_time": "2019-11-25T19:18:26.985066Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    (amrozi, accused, his, brother, ,, whom, he, c...\n",
       "1    (yucaipa, owned, dominick, 's, before, selling...\n",
       "2    (they, had, published, an, advertisement, on, ...\n",
       "3    (around, 0335, gmt, ,, tab, shares, were, up, ...\n",
       "4    (the, stock, rose, $, 2.11, ,, or, about, 11, ...\n",
       "dtype: object"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concat_docs = pd.concat([text[\"doc_1_proc\"], text[\"doc_2_proc\"]], axis=0)\n",
    "concat_docs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-25T19:23:52.578919Z",
     "start_time": "2019-11-25T19:23:52.430346Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# build gensim corpus\n",
    "corpus_docs = []\n",
    "for i, doc in enumerate(concat_docs):\n",
    "    corpus_docs.append([token.text for token in doc if not token.is_stop and not token.is_punct])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-25T20:29:16.899549Z",
     "start_time": "2019-11-25T20:29:16.889989Z"
    }
   },
   "source": [
    "A quite useful metric is the earth mover distance, accessible via `doc2vec_model.wv.wmdistance`. EMD accounts for the problem of synonyms without literal overlap (which other metrics, e.g. Jaccard distance, cannot do) by seeking the smallest distance between distributions of word clusters in the embedding space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-25T20:11:50.623698Z",
     "start_time": "2019-11-25T20:11:50.606144Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1125 20:11:50.620370 140283058251648 base_any2vec.py:723] consider setting layer size to a multiple of 4 for greater performance\n"
     ]
    }
   ],
   "source": [
    "doc2vec_documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(corpus_docs)]\n",
    "doc2vec_model = Doc2Vec(vector_size=50, window=4, min_count=1, workers=4, epochs=40) # instantiate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-25T20:12:28.496082Z",
     "start_time": "2019-11-25T20:12:28.036940Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1125 20:12:28.039155 140283058251648 doc2vec.py:1377] collecting all words and their counts\n",
      "I1125 20:12:28.040684 140283058251648 doc2vec.py:1321] PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags\n",
      "I1125 20:12:28.076210 140283058251648 doc2vec.py:1385] collected 12844 word types and 6916 unique tags from a corpus of 6916 examples and 79407 words\n",
      "I1125 20:12:28.077234 140283058251648 word2vec.py:1647] Loading a fresh vocabulary\n",
      "I1125 20:12:28.120304 140283058251648 word2vec.py:1671] effective_min_count=1 retains 12844 unique words (100% of original 12844, drops 0)\n",
      "I1125 20:12:28.121589 140283058251648 word2vec.py:1677] effective_min_count=1 leaves 79407 word corpus (100% of original 79407, drops 0)\n",
      "I1125 20:12:28.215401 140283058251648 word2vec.py:1736] deleting the raw counts dictionary of 12844 items\n",
      "I1125 20:12:28.217094 140283058251648 word2vec.py:1739] sample=0.001 downsamples 8 most-common words\n",
      "I1125 20:12:28.218403 140283058251648 word2vec.py:1742] downsampling leaves estimated 76998 word corpus (97.0% of prior 79407)\n",
      "I1125 20:12:28.263919 140283058251648 base_any2vec.py:1022] estimated required memory for 12844 words and 50 dimensions: 12942800 bytes\n",
      "I1125 20:12:28.264826 140283058251648 word2vec.py:1888] resetting layer weights\n"
     ]
    }
   ],
   "source": [
    "# fit the model to the vocabulary\n",
    "doc2vec_model.build_vocab(doc2vec_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-25T20:14:03.632401Z",
     "start_time": "2019-11-25T20:13:49.387716Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1125 20:13:49.388809 140283058251648 base_any2vec.py:1210] training model with 4 workers on 12844 vocabulary and 50 features, using sg=0 hs=0 sample=0.001 negative=5 window=4\n",
      "I1125 20:13:49.683892 140283058251648 base_any2vec.py:349] worker thread finished; awaiting finish of 3 more threads\n",
      "I1125 20:13:49.690744 140283058251648 base_any2vec.py:349] worker thread finished; awaiting finish of 2 more threads\n",
      "I1125 20:13:49.708986 140283058251648 base_any2vec.py:349] worker thread finished; awaiting finish of 1 more threads\n",
      "I1125 20:13:49.722156 140283058251648 base_any2vec.py:349] worker thread finished; awaiting finish of 0 more threads\n",
      "I1125 20:13:49.723231 140283058251648 base_any2vec.py:1346] EPOCH - 1 : training on 79407 raw words (83840 effective words) took 0.3s, 267215 effective words/s\n",
      "I1125 20:13:49.978583 140283058251648 base_any2vec.py:349] worker thread finished; awaiting finish of 3 more threads\n",
      "I1125 20:13:50.014454 140283058251648 base_any2vec.py:349] worker thread finished; awaiting finish of 2 more threads\n",
      "I1125 20:13:50.019215 140283058251648 base_any2vec.py:349] worker thread finished; awaiting finish of 1 more threads\n",
      "I1125 20:13:50.030811 140283058251648 base_any2vec.py:349] worker thread finished; awaiting finish of 0 more threads\n",
      "I1125 20:13:50.031755 140283058251648 base_any2vec.py:1346] EPOCH - 2 : training on 79407 raw words (83937 effective words) took 0.3s, 286753 effective words/s\n",
      "I1125 20:13:50.337533 140283058251648 base_any2vec.py:349] worker thread finished; awaiting finish of 3 more threads\n",
      "I1125 20:13:50.368020 140283058251648 base_any2vec.py:349] worker thread finished; awaiting finish of 2 more threads\n",
      "I1125 20:13:50.377847 140283058251648 base_any2vec.py:349] worker thread finished; awaiting finish of 1 more threads\n",
      "I1125 20:13:50.384908 140283058251648 base_any2vec.py:349] worker thread finished; awaiting finish of 0 more threads\n",
      "I1125 20:13:50.385812 140283058251648 base_any2vec.py:1346] EPOCH - 3 : training on 79407 raw words (83867 effective words) took 0.3s, 248890 effective words/s\n",
      "I1125 20:13:50.707569 140283058251648 base_any2vec.py:349] worker thread finished; awaiting finish of 3 more threads\n",
      "I1125 20:13:50.727667 140283058251648 base_any2vec.py:349] worker thread finished; awaiting finish of 2 more threads\n",
      "I1125 20:13:50.736191 140283058251648 base_any2vec.py:349] worker thread finished; awaiting finish of 1 more threads\n",
      "I1125 20:13:50.741873 140283058251648 base_any2vec.py:349] worker thread finished; awaiting finish of 0 more threads\n",
      "I1125 20:13:50.742829 140283058251648 base_any2vec.py:1346] EPOCH - 4 : training on 79407 raw words (83881 effective words) took 0.3s, 240927 effective words/s\n",
      "I1125 20:13:51.039461 140283058251648 base_any2vec.py:349] worker thread finished; awaiting finish of 3 more threads\n",
      "I1125 20:13:51.052804 140283058251648 base_any2vec.py:349] worker thread finished; awaiting finish of 2 more threads\n",
      "I1125 20:13:51.063031 140283058251648 base_any2vec.py:349] worker thread finished; awaiting finish of 1 more threads\n",
      "I1125 20:13:51.072535 140283058251648 base_any2vec.py:349] worker thread finished; awaiting finish of 0 more threads\n",
      "I1125 20:13:51.073460 140283058251648 base_any2vec.py:1346] EPOCH - 5 : training on 79407 raw words (83921 effective words) took 0.3s, 261345 effective words/s\n",
      "I1125 20:13:51.414618 140283058251648 base_any2vec.py:349] worker thread finished; awaiting finish of 3 more threads\n",
      "I1125 20:13:51.439290 140283058251648 base_any2vec.py:349] worker thread finished; awaiting finish of 2 more threads\n",
      "I1125 20:13:51.451796 140283058251648 base_any2vec.py:349] worker thread finished; awaiting finish of 1 more threads\n",
      "I1125 20:13:51.471836 140283058251648 base_any2vec.py:349] worker thread finished; awaiting finish of 0 more threads\n",
      "I1125 20:13:51.472870 140283058251648 base_any2vec.py:1346] EPOCH - 6 : training on 79407 raw words (83918 effective words) took 0.4s, 224511 effective words/s\n",
      "I1125 20:13:51.840250 140283058251648 base_any2vec.py:349] worker thread finished; awaiting finish of 3 more threads\n",
      "I1125 20:13:51.844236 140283058251648 base_any2vec.py:349] worker thread finished; awaiting finish of 2 more threads\n",
      "I1125 20:13:51.853245 140283058251648 base_any2vec.py:349] worker thread finished; awaiting finish of 1 more threads\n",
      "I1125 20:13:51.862091 140283058251648 base_any2vec.py:349] worker thread finished; awaiting finish of 0 more threads\n",
      "I1125 20:13:51.863259 140283058251648 base_any2vec.py:1346] EPOCH - 7 : training on 79407 raw words (83979 effective words) took 0.4s, 224817 effective words/s\n",
      "I1125 20:13:52.253095 140283058251648 base_any2vec.py:349] worker thread finished; awaiting finish of 3 more threads\n",
      "I1125 20:13:52.272599 140283058251648 base_any2vec.py:349] worker thread finished; awaiting finish of 2 more threads\n",
      "I1125 20:13:52.278434 140283058251648 base_any2vec.py:349] worker thread finished; awaiting finish of 1 more threads\n",
      "I1125 20:13:52.281997 140283058251648 base_any2vec.py:349] worker thread finished; awaiting finish of 0 more threads\n",
      "I1125 20:13:52.283100 140283058251648 base_any2vec.py:1346] EPOCH - 8 : training on 79407 raw words (83922 effective words) took 0.4s, 207822 effective words/s\n",
      "I1125 20:13:52.660400 140283058251648 base_any2vec.py:349] worker thread finished; awaiting finish of 3 more threads\n",
      "I1125 20:13:52.667897 140283058251648 base_any2vec.py:349] worker thread finished; awaiting finish of 2 more threads\n",
      "I1125 20:13:52.674083 140283058251648 base_any2vec.py:349] worker thread finished; awaiting finish of 1 more threads\n",
      "I1125 20:13:52.679674 140283058251648 base_any2vec.py:349] worker thread finished; awaiting finish of 0 more threads\n",
      "I1125 20:13:52.680936 140283058251648 base_any2vec.py:1346] EPOCH - 9 : training on 79407 raw words (83918 effective words) took 0.4s, 215133 effective words/s\n",
      "I1125 20:13:53.038864 140283058251648 base_any2vec.py:349] worker thread finished; awaiting finish of 3 more threads\n",
      "I1125 20:13:53.052681 140283058251648 base_any2vec.py:349] worker thread finished; awaiting finish of 2 more threads\n",
      "I1125 20:13:53.061929 140283058251648 base_any2vec.py:349] worker thread finished; awaiting finish of 1 more threads\n",
      "I1125 20:13:53.069216 140283058251648 base_any2vec.py:349] worker thread finished; awaiting finish of 0 more threads\n",
      "I1125 20:13:53.070280 140283058251648 base_any2vec.py:1346] EPOCH - 10 : training on 79407 raw words (83860 effective words) took 0.4s, 219968 effective words/s\n",
      "I1125 20:13:53.415853 140283058251648 base_any2vec.py:349] worker thread finished; awaiting finish of 3 more threads\n",
      "I1125 20:13:53.422105 140283058251648 base_any2vec.py:349] worker thread finished; awaiting finish of 2 more threads\n",
      "I1125 20:13:53.427885 140283058251648 base_any2vec.py:349] worker thread finished; awaiting finish of 1 more threads\n",
      "I1125 20:13:53.431392 140283058251648 base_any2vec.py:349] worker thread finished; awaiting finish of 0 more threads\n",
      "I1125 20:13:53.432462 140283058251648 base_any2vec.py:1346] EPOCH - 11 : training on 79407 raw words (83909 effective words) took 0.4s, 237774 effective words/s\n",
      "I1125 20:13:53.776274 140283058251648 base_any2vec.py:349] worker thread finished; awaiting finish of 3 more threads\n",
      "I1125 20:13:53.787599 140283058251648 base_any2vec.py:349] worker thread finished; awaiting finish of 2 more threads\n",
      "I1125 20:13:53.792867 140283058251648 base_any2vec.py:349] worker thread finished; awaiting finish of 1 more threads\n",
      "I1125 20:13:53.798189 140283058251648 base_any2vec.py:349] worker thread finished; awaiting finish of 0 more threads\n",
      "I1125 20:13:53.799439 140283058251648 base_any2vec.py:1346] EPOCH - 12 : training on 79407 raw words (83953 effective words) took 0.3s, 246590 effective words/s\n",
      "I1125 20:13:54.122518 140283058251648 base_any2vec.py:349] worker thread finished; awaiting finish of 3 more threads\n",
      "I1125 20:13:54.130187 140283058251648 base_any2vec.py:349] worker thread finished; awaiting finish of 2 more threads\n",
      "I1125 20:13:54.135317 140283058251648 base_any2vec.py:349] worker thread finished; awaiting finish of 1 more threads\n",
      "I1125 20:13:54.140146 140283058251648 base_any2vec.py:349] worker thread finished; awaiting finish of 0 more threads\n",
      "I1125 20:13:54.141002 140283058251648 base_any2vec.py:1346] EPOCH - 13 : training on 79407 raw words (83864 effective words) took 0.3s, 253212 effective words/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1125 20:13:54.483043 140283058251648 base_any2vec.py:349] worker thread finished; awaiting finish of 3 more threads\n",
      "I1125 20:13:54.497586 140283058251648 base_any2vec.py:349] worker thread finished; awaiting finish of 2 more threads\n",
      "I1125 20:13:54.508027 140283058251648 base_any2vec.py:349] worker thread finished; awaiting finish of 1 more threads\n",
      "I1125 20:13:54.514766 140283058251648 base_any2vec.py:349] worker thread finished; awaiting finish of 0 more threads\n",
      "I1125 20:13:54.516225 140283058251648 base_any2vec.py:1346] EPOCH - 14 : training on 79407 raw words (83961 effective words) took 0.4s, 235061 effective words/s\n",
      "I1125 20:13:54.880956 140283058251648 base_any2vec.py:349] worker thread finished; awaiting finish of 3 more threads\n",
      "I1125 20:13:54.893553 140283058251648 base_any2vec.py:349] worker thread finished; awaiting finish of 2 more threads\n",
      "I1125 20:13:54.899285 140283058251648 base_any2vec.py:349] worker thread finished; awaiting finish of 1 more threads\n",
      "I1125 20:13:54.907469 140283058251648 base_any2vec.py:349] worker thread finished; awaiting finish of 0 more threads\n",
      "I1125 20:13:54.908554 140283058251648 base_any2vec.py:1346] EPOCH - 15 : training on 79407 raw words (83927 effective words) took 0.4s, 227031 effective words/s\n",
      "I1125 20:13:55.254940 140283058251648 base_any2vec.py:349] worker thread finished; awaiting finish of 3 more threads\n",
      "I1125 20:13:55.269860 140283058251648 base_any2vec.py:349] worker thread finished; awaiting finish of 2 more threads\n",
      "I1125 20:13:55.296017 140283058251648 base_any2vec.py:349] worker thread finished; awaiting finish of 1 more threads\n",
      "I1125 20:13:55.299168 140283058251648 base_any2vec.py:349] worker thread finished; awaiting finish of 0 more threads\n",
      "I1125 20:13:55.300117 140283058251648 base_any2vec.py:1346] EPOCH - 16 : training on 79407 raw words (83908 effective words) took 0.4s, 226197 effective words/s\n",
      "I1125 20:13:55.663517 140283058251648 base_any2vec.py:349] worker thread finished; awaiting finish of 3 more threads\n",
      "I1125 20:13:55.672301 140283058251648 base_any2vec.py:349] worker thread finished; awaiting finish of 2 more threads\n",
      "I1125 20:13:55.685091 140283058251648 base_any2vec.py:349] worker thread finished; awaiting finish of 1 more threads\n",
      "I1125 20:13:55.704252 140283058251648 base_any2vec.py:349] worker thread finished; awaiting finish of 0 more threads\n",
      "I1125 20:13:55.705238 140283058251648 base_any2vec.py:1346] EPOCH - 17 : training on 79407 raw words (83938 effective words) took 0.4s, 217398 effective words/s\n",
      "I1125 20:13:56.056589 140283058251648 base_any2vec.py:349] worker thread finished; awaiting finish of 3 more threads\n",
      "I1125 20:13:56.074427 140283058251648 base_any2vec.py:349] worker thread finished; awaiting finish of 2 more threads\n",
      "I1125 20:13:56.091150 140283058251648 base_any2vec.py:349] worker thread finished; awaiting finish of 1 more threads\n",
      "I1125 20:13:56.095514 140283058251648 base_any2vec.py:349] worker thread finished; awaiting finish of 0 more threads\n",
      "I1125 20:13:56.096892 140283058251648 base_any2vec.py:1346] EPOCH - 18 : training on 79407 raw words (83892 effective words) took 0.4s, 219152 effective words/s\n",
      "I1125 20:13:56.457185 140283058251648 base_any2vec.py:349] worker thread finished; awaiting finish of 3 more threads\n",
      "I1125 20:13:56.460469 140283058251648 base_any2vec.py:349] worker thread finished; awaiting finish of 2 more threads\n",
      "I1125 20:13:56.468254 140283058251648 base_any2vec.py:349] worker thread finished; awaiting finish of 1 more threads\n",
      "I1125 20:13:56.481104 140283058251648 base_any2vec.py:349] worker thread finished; awaiting finish of 0 more threads\n",
      "I1125 20:13:56.483447 140283058251648 base_any2vec.py:1346] EPOCH - 19 : training on 79407 raw words (83895 effective words) took 0.4s, 222762 effective words/s\n",
      "I1125 20:13:56.828453 140283058251648 base_any2vec.py:349] worker thread finished; awaiting finish of 3 more threads\n",
      "I1125 20:13:56.846897 140283058251648 base_any2vec.py:349] worker thread finished; awaiting finish of 2 more threads\n",
      "I1125 20:13:56.851210 140283058251648 base_any2vec.py:349] worker thread finished; awaiting finish of 1 more threads\n",
      "I1125 20:13:56.867316 140283058251648 base_any2vec.py:349] worker thread finished; awaiting finish of 0 more threads\n",
      "I1125 20:13:56.868518 140283058251648 base_any2vec.py:1346] EPOCH - 20 : training on 79407 raw words (83931 effective words) took 0.4s, 227090 effective words/s\n",
      "I1125 20:13:57.196037 140283058251648 base_any2vec.py:349] worker thread finished; awaiting finish of 3 more threads\n",
      "I1125 20:13:57.205333 140283058251648 base_any2vec.py:349] worker thread finished; awaiting finish of 2 more threads\n",
      "I1125 20:13:57.212933 140283058251648 base_any2vec.py:349] worker thread finished; awaiting finish of 1 more threads\n",
      "I1125 20:13:57.219121 140283058251648 base_any2vec.py:349] worker thread finished; awaiting finish of 0 more threads\n",
      "I1125 20:13:57.220141 140283058251648 base_any2vec.py:1346] EPOCH - 21 : training on 79407 raw words (83900 effective words) took 0.3s, 244845 effective words/s\n",
      "I1125 20:13:57.554251 140283058251648 base_any2vec.py:349] worker thread finished; awaiting finish of 3 more threads\n",
      "I1125 20:13:57.577648 140283058251648 base_any2vec.py:349] worker thread finished; awaiting finish of 2 more threads\n",
      "I1125 20:13:57.579572 140283058251648 base_any2vec.py:349] worker thread finished; awaiting finish of 1 more threads\n",
      "I1125 20:13:57.581720 140283058251648 base_any2vec.py:349] worker thread finished; awaiting finish of 0 more threads\n",
      "I1125 20:13:57.582648 140283058251648 base_any2vec.py:1346] EPOCH - 22 : training on 79407 raw words (83957 effective words) took 0.3s, 244937 effective words/s\n",
      "I1125 20:13:57.859409 140283058251648 base_any2vec.py:349] worker thread finished; awaiting finish of 3 more threads\n",
      "I1125 20:13:57.878076 140283058251648 base_any2vec.py:349] worker thread finished; awaiting finish of 2 more threads\n",
      "I1125 20:13:57.893395 140283058251648 base_any2vec.py:349] worker thread finished; awaiting finish of 1 more threads\n",
      "I1125 20:13:57.902358 140283058251648 base_any2vec.py:349] worker thread finished; awaiting finish of 0 more threads\n",
      "I1125 20:13:57.903570 140283058251648 base_any2vec.py:1346] EPOCH - 23 : training on 79407 raw words (83889 effective words) took 0.3s, 285606 effective words/s\n",
      "I1125 20:13:58.226759 140283058251648 base_any2vec.py:349] worker thread finished; awaiting finish of 3 more threads\n",
      "I1125 20:13:58.237660 140283058251648 base_any2vec.py:349] worker thread finished; awaiting finish of 2 more threads\n",
      "I1125 20:13:58.246658 140283058251648 base_any2vec.py:349] worker thread finished; awaiting finish of 1 more threads\n",
      "I1125 20:13:58.248357 140283058251648 base_any2vec.py:349] worker thread finished; awaiting finish of 0 more threads\n",
      "I1125 20:13:58.249454 140283058251648 base_any2vec.py:1346] EPOCH - 24 : training on 79407 raw words (83933 effective words) took 0.3s, 261980 effective words/s\n",
      "I1125 20:13:58.547726 140283058251648 base_any2vec.py:349] worker thread finished; awaiting finish of 3 more threads\n",
      "I1125 20:13:58.593483 140283058251648 base_any2vec.py:349] worker thread finished; awaiting finish of 2 more threads\n",
      "I1125 20:13:58.597734 140283058251648 base_any2vec.py:349] worker thread finished; awaiting finish of 1 more threads\n",
      "I1125 20:13:58.605706 140283058251648 base_any2vec.py:349] worker thread finished; awaiting finish of 0 more threads\n",
      "I1125 20:13:58.606748 140283058251648 base_any2vec.py:1346] EPOCH - 25 : training on 79407 raw words (83917 effective words) took 0.3s, 256105 effective words/s\n",
      "I1125 20:13:58.894244 140283058251648 base_any2vec.py:349] worker thread finished; awaiting finish of 3 more threads\n",
      "I1125 20:13:58.910237 140283058251648 base_any2vec.py:349] worker thread finished; awaiting finish of 2 more threads\n",
      "I1125 20:13:58.928186 140283058251648 base_any2vec.py:349] worker thread finished; awaiting finish of 1 more threads\n",
      "I1125 20:13:58.939855 140283058251648 base_any2vec.py:349] worker thread finished; awaiting finish of 0 more threads\n",
      "I1125 20:13:58.940780 140283058251648 base_any2vec.py:1346] EPOCH - 26 : training on 79407 raw words (83906 effective words) took 0.3s, 260883 effective words/s\n",
      "I1125 20:13:59.268069 140283058251648 base_any2vec.py:349] worker thread finished; awaiting finish of 3 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1125 20:13:59.289099 140283058251648 base_any2vec.py:349] worker thread finished; awaiting finish of 2 more threads\n",
      "I1125 20:13:59.306787 140283058251648 base_any2vec.py:349] worker thread finished; awaiting finish of 1 more threads\n",
      "I1125 20:13:59.318450 140283058251648 base_any2vec.py:349] worker thread finished; awaiting finish of 0 more threads\n",
      "I1125 20:13:59.319638 140283058251648 base_any2vec.py:1346] EPOCH - 27 : training on 79407 raw words (83937 effective words) took 0.4s, 232825 effective words/s\n",
      "I1125 20:13:59.592310 140283058251648 base_any2vec.py:349] worker thread finished; awaiting finish of 3 more threads\n",
      "I1125 20:13:59.629797 140283058251648 base_any2vec.py:349] worker thread finished; awaiting finish of 2 more threads\n",
      "I1125 20:13:59.638214 140283058251648 base_any2vec.py:349] worker thread finished; awaiting finish of 1 more threads\n",
      "I1125 20:13:59.644656 140283058251648 base_any2vec.py:349] worker thread finished; awaiting finish of 0 more threads\n",
      "I1125 20:13:59.645578 140283058251648 base_any2vec.py:1346] EPOCH - 28 : training on 79407 raw words (83901 effective words) took 0.3s, 273734 effective words/s\n",
      "I1125 20:13:59.939245 140283058251648 base_any2vec.py:349] worker thread finished; awaiting finish of 3 more threads\n",
      "I1125 20:13:59.954811 140283058251648 base_any2vec.py:349] worker thread finished; awaiting finish of 2 more threads\n",
      "I1125 20:13:59.969134 140283058251648 base_any2vec.py:349] worker thread finished; awaiting finish of 1 more threads\n",
      "I1125 20:13:59.974365 140283058251648 base_any2vec.py:349] worker thread finished; awaiting finish of 0 more threads\n",
      "I1125 20:13:59.975352 140283058251648 base_any2vec.py:1346] EPOCH - 29 : training on 79407 raw words (83886 effective words) took 0.3s, 261570 effective words/s\n",
      "I1125 20:14:00.285152 140283058251648 base_any2vec.py:349] worker thread finished; awaiting finish of 3 more threads\n",
      "I1125 20:14:00.288713 140283058251648 base_any2vec.py:349] worker thread finished; awaiting finish of 2 more threads\n",
      "I1125 20:14:00.293484 140283058251648 base_any2vec.py:349] worker thread finished; awaiting finish of 1 more threads\n",
      "I1125 20:14:00.298805 140283058251648 base_any2vec.py:349] worker thread finished; awaiting finish of 0 more threads\n",
      "I1125 20:14:00.299761 140283058251648 base_any2vec.py:1346] EPOCH - 30 : training on 79407 raw words (83953 effective words) took 0.3s, 264671 effective words/s\n",
      "I1125 20:14:00.604774 140283058251648 base_any2vec.py:349] worker thread finished; awaiting finish of 3 more threads\n",
      "I1125 20:14:00.615889 140283058251648 base_any2vec.py:349] worker thread finished; awaiting finish of 2 more threads\n",
      "I1125 20:14:00.623703 140283058251648 base_any2vec.py:349] worker thread finished; awaiting finish of 1 more threads\n",
      "I1125 20:14:00.629397 140283058251648 base_any2vec.py:349] worker thread finished; awaiting finish of 0 more threads\n",
      "I1125 20:14:00.630308 140283058251648 base_any2vec.py:1346] EPOCH - 31 : training on 79407 raw words (83889 effective words) took 0.3s, 270399 effective words/s\n",
      "I1125 20:14:00.971976 140283058251648 base_any2vec.py:349] worker thread finished; awaiting finish of 3 more threads\n",
      "I1125 20:14:00.982746 140283058251648 base_any2vec.py:349] worker thread finished; awaiting finish of 2 more threads\n",
      "I1125 20:14:00.985153 140283058251648 base_any2vec.py:349] worker thread finished; awaiting finish of 1 more threads\n",
      "I1125 20:14:00.990134 140283058251648 base_any2vec.py:349] worker thread finished; awaiting finish of 0 more threads\n",
      "I1125 20:14:00.991041 140283058251648 base_any2vec.py:1346] EPOCH - 32 : training on 79407 raw words (83886 effective words) took 0.3s, 245689 effective words/s\n",
      "I1125 20:14:01.294837 140283058251648 base_any2vec.py:349] worker thread finished; awaiting finish of 3 more threads\n",
      "I1125 20:14:01.308136 140283058251648 base_any2vec.py:349] worker thread finished; awaiting finish of 2 more threads\n",
      "I1125 20:14:01.314101 140283058251648 base_any2vec.py:349] worker thread finished; awaiting finish of 1 more threads\n",
      "I1125 20:14:01.317797 140283058251648 base_any2vec.py:349] worker thread finished; awaiting finish of 0 more threads\n",
      "I1125 20:14:01.318602 140283058251648 base_any2vec.py:1346] EPOCH - 33 : training on 79407 raw words (83983 effective words) took 0.3s, 253892 effective words/s\n",
      "I1125 20:14:01.634112 140283058251648 base_any2vec.py:349] worker thread finished; awaiting finish of 3 more threads\n",
      "I1125 20:14:01.645140 140283058251648 base_any2vec.py:349] worker thread finished; awaiting finish of 2 more threads\n",
      "I1125 20:14:01.647982 140283058251648 base_any2vec.py:349] worker thread finished; awaiting finish of 1 more threads\n",
      "I1125 20:14:01.650948 140283058251648 base_any2vec.py:349] worker thread finished; awaiting finish of 0 more threads\n",
      "I1125 20:14:01.651711 140283058251648 base_any2vec.py:1346] EPOCH - 34 : training on 79407 raw words (83996 effective words) took 0.3s, 268122 effective words/s\n",
      "I1125 20:14:01.986571 140283058251648 base_any2vec.py:349] worker thread finished; awaiting finish of 3 more threads\n",
      "I1125 20:14:01.997459 140283058251648 base_any2vec.py:349] worker thread finished; awaiting finish of 2 more threads\n",
      "I1125 20:14:02.001973 140283058251648 base_any2vec.py:349] worker thread finished; awaiting finish of 1 more threads\n",
      "I1125 20:14:02.014017 140283058251648 base_any2vec.py:349] worker thread finished; awaiting finish of 0 more threads\n",
      "I1125 20:14:02.015285 140283058251648 base_any2vec.py:1346] EPOCH - 35 : training on 79407 raw words (83893 effective words) took 0.3s, 241511 effective words/s\n",
      "I1125 20:14:02.337303 140283058251648 base_any2vec.py:349] worker thread finished; awaiting finish of 3 more threads\n",
      "I1125 20:14:02.360721 140283058251648 base_any2vec.py:349] worker thread finished; awaiting finish of 2 more threads\n",
      "I1125 20:14:02.364293 140283058251648 base_any2vec.py:349] worker thread finished; awaiting finish of 1 more threads\n",
      "I1125 20:14:02.371016 140283058251648 base_any2vec.py:349] worker thread finished; awaiting finish of 0 more threads\n",
      "I1125 20:14:02.372631 140283058251648 base_any2vec.py:1346] EPOCH - 36 : training on 79407 raw words (83871 effective words) took 0.3s, 240654 effective words/s\n",
      "I1125 20:14:02.703058 140283058251648 base_any2vec.py:349] worker thread finished; awaiting finish of 3 more threads\n",
      "I1125 20:14:02.707893 140283058251648 base_any2vec.py:349] worker thread finished; awaiting finish of 2 more threads\n",
      "I1125 20:14:02.713389 140283058251648 base_any2vec.py:349] worker thread finished; awaiting finish of 1 more threads\n",
      "I1125 20:14:02.718935 140283058251648 base_any2vec.py:349] worker thread finished; awaiting finish of 0 more threads\n",
      "I1125 20:14:02.719797 140283058251648 base_any2vec.py:1346] EPOCH - 37 : training on 79407 raw words (83910 effective words) took 0.3s, 249322 effective words/s\n",
      "I1125 20:14:03.006965 140283058251648 base_any2vec.py:349] worker thread finished; awaiting finish of 3 more threads\n",
      "I1125 20:14:03.014729 140283058251648 base_any2vec.py:349] worker thread finished; awaiting finish of 2 more threads\n",
      "I1125 20:14:03.016873 140283058251648 base_any2vec.py:349] worker thread finished; awaiting finish of 1 more threads\n",
      "I1125 20:14:03.020175 140283058251648 base_any2vec.py:349] worker thread finished; awaiting finish of 0 more threads\n",
      "I1125 20:14:03.021090 140283058251648 base_any2vec.py:1346] EPOCH - 38 : training on 79407 raw words (83945 effective words) took 0.3s, 306964 effective words/s\n",
      "I1125 20:14:03.306587 140283058251648 base_any2vec.py:349] worker thread finished; awaiting finish of 3 more threads\n",
      "I1125 20:14:03.312014 140283058251648 base_any2vec.py:349] worker thread finished; awaiting finish of 2 more threads\n",
      "I1125 20:14:03.315096 140283058251648 base_any2vec.py:349] worker thread finished; awaiting finish of 1 more threads\n",
      "I1125 20:14:03.318460 140283058251648 base_any2vec.py:349] worker thread finished; awaiting finish of 0 more threads\n",
      "I1125 20:14:03.319206 140283058251648 base_any2vec.py:1346] EPOCH - 39 : training on 79407 raw words (83946 effective words) took 0.3s, 298002 effective words/s\n",
      "I1125 20:14:03.614537 140283058251648 base_any2vec.py:349] worker thread finished; awaiting finish of 3 more threads\n",
      "I1125 20:14:03.617970 140283058251648 base_any2vec.py:349] worker thread finished; awaiting finish of 2 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1125 20:14:03.622956 140283058251648 base_any2vec.py:349] worker thread finished; awaiting finish of 1 more threads\n",
      "I1125 20:14:03.628952 140283058251648 base_any2vec.py:349] worker thread finished; awaiting finish of 0 more threads\n",
      "I1125 20:14:03.629824 140283058251648 base_any2vec.py:1346] EPOCH - 40 : training on 79407 raw words (83932 effective words) took 0.3s, 279151 effective words/s\n",
      "I1125 20:14:03.630399 140283058251648 base_any2vec.py:1382] training on a 3176280 raw words (3356651 effective words) took 14.3s, 235356 effective words/s\n"
     ]
    }
   ],
   "source": [
    "# train the model on the training docs\n",
    "doc2vec_model.train(doc2vec_documents, total_examples=doc2vec_model.corpus_count, epochs=doc2vec_model.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-25T20:09:04.322768Z",
     "start_time": "2019-11-25T20:09:04.319545Z"
    }
   },
   "outputs": [],
   "source": [
    "# from gensim.test.utils import get_tmpfile\n",
    "\n",
    "# file_name = get_tmpfile(\"doc2vec_model\")\n",
    "\n",
    "# model.save(file_name)\n",
    "\n",
    "# To continue training online, we can re-load the model: \n",
    "\n",
    "# doc2vec_model = Doc2Vec.load(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-25T19:40:45.312533Z",
     "start_time": "2019-11-25T19:40:45.106409Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1125 19:40:45.107861 140283058251648 dictionary.py:205] adding document #0 to Dictionary(0 unique tokens: [])\n",
      "I1125 19:40:45.309946 140283058251648 dictionary.py:212] built Dictionary(12844 unique tokens: ['accused', 'amrozi', 'brother', 'called', 'deliberately']...) from 6916 documents (total 79407 corpus positions)\n"
     ]
    }
   ],
   "source": [
    "dictionary = corpora.Dictionary(corpus_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-25T19:55:40.964952Z",
     "start_time": "2019-11-25T19:55:38.612411Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1125 19:55:38.614854 140283058251648 word2vec.py:1588] collecting all words and their counts\n",
      "I1125 19:55:38.616070 140283058251648 word2vec.py:1573] PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "I1125 19:55:38.638021 140283058251648 word2vec.py:1596] collected 12844 word types from a corpus of 79407 raw words and 6916 sentences\n",
      "I1125 19:55:38.638849 140283058251648 word2vec.py:1647] Loading a fresh vocabulary\n",
      "I1125 19:55:38.669111 140283058251648 word2vec.py:1671] effective_min_count=1 retains 12844 unique words (100% of original 12844, drops 0)\n",
      "I1125 19:55:38.670149 140283058251648 word2vec.py:1677] effective_min_count=1 leaves 79407 word corpus (100% of original 79407, drops 0)\n",
      "I1125 19:55:38.732173 140283058251648 word2vec.py:1736] deleting the raw counts dictionary of 12844 items\n",
      "I1125 19:55:38.733381 140283058251648 word2vec.py:1739] sample=0.001 downsamples 8 most-common words\n",
      "I1125 19:55:38.734551 140283058251648 word2vec.py:1742] downsampling leaves estimated 76998 word corpus (97.0% of prior 79407)\n",
      "I1125 19:55:38.775052 140283058251648 base_any2vec.py:1022] estimated required memory for 12844 words and 100 dimensions: 16697200 bytes\n",
      "I1125 19:55:38.776052 140283058251648 word2vec.py:1888] resetting layer weights\n",
      "I1125 19:55:38.947696 140283058251648 base_any2vec.py:1210] training model with 4 workers on 12844 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "I1125 19:55:39.121700 140283058251648 base_any2vec.py:349] worker thread finished; awaiting finish of 3 more threads\n",
      "I1125 19:55:39.122766 140283058251648 base_any2vec.py:349] worker thread finished; awaiting finish of 2 more threads\n",
      "I1125 19:55:39.123817 140283058251648 base_any2vec.py:349] worker thread finished; awaiting finish of 1 more threads\n",
      "I1125 19:55:39.124619 140283058251648 base_any2vec.py:349] worker thread finished; awaiting finish of 0 more threads\n",
      "I1125 19:55:39.125362 140283058251648 base_any2vec.py:1346] EPOCH - 1 : training on 79407 raw words (76981 effective words) took 0.2s, 471193 effective words/s\n",
      "I1125 19:55:39.313728 140283058251648 base_any2vec.py:349] worker thread finished; awaiting finish of 3 more threads\n",
      "I1125 19:55:39.315300 140283058251648 base_any2vec.py:349] worker thread finished; awaiting finish of 2 more threads\n",
      "I1125 19:55:39.316870 140283058251648 base_any2vec.py:349] worker thread finished; awaiting finish of 1 more threads\n",
      "I1125 19:55:39.317552 140283058251648 base_any2vec.py:349] worker thread finished; awaiting finish of 0 more threads\n",
      "I1125 19:55:39.318163 140283058251648 base_any2vec.py:1346] EPOCH - 2 : training on 79407 raw words (76992 effective words) took 0.2s, 433553 effective words/s\n",
      "I1125 19:55:39.498357 140283058251648 base_any2vec.py:349] worker thread finished; awaiting finish of 3 more threads\n",
      "I1125 19:55:39.500047 140283058251648 base_any2vec.py:349] worker thread finished; awaiting finish of 2 more threads\n",
      "I1125 19:55:39.501493 140283058251648 base_any2vec.py:349] worker thread finished; awaiting finish of 1 more threads\n",
      "I1125 19:55:39.502624 140283058251648 base_any2vec.py:349] worker thread finished; awaiting finish of 0 more threads\n",
      "I1125 19:55:39.503366 140283058251648 base_any2vec.py:1346] EPOCH - 3 : training on 79407 raw words (77014 effective words) took 0.2s, 460455 effective words/s\n",
      "I1125 19:55:39.685193 140283058251648 base_any2vec.py:349] worker thread finished; awaiting finish of 3 more threads\n",
      "I1125 19:55:39.686937 140283058251648 base_any2vec.py:349] worker thread finished; awaiting finish of 2 more threads\n",
      "I1125 19:55:39.687615 140283058251648 base_any2vec.py:349] worker thread finished; awaiting finish of 1 more threads\n",
      "I1125 19:55:39.689354 140283058251648 base_any2vec.py:349] worker thread finished; awaiting finish of 0 more threads\n",
      "I1125 19:55:39.690134 140283058251648 base_any2vec.py:1346] EPOCH - 4 : training on 79407 raw words (77007 effective words) took 0.2s, 451266 effective words/s\n",
      "I1125 19:55:39.885577 140283058251648 base_any2vec.py:349] worker thread finished; awaiting finish of 3 more threads\n",
      "I1125 19:55:39.887388 140283058251648 base_any2vec.py:349] worker thread finished; awaiting finish of 2 more threads\n",
      "I1125 19:55:39.888884 140283058251648 base_any2vec.py:349] worker thread finished; awaiting finish of 1 more threads\n",
      "I1125 19:55:39.890208 140283058251648 base_any2vec.py:349] worker thread finished; awaiting finish of 0 more threads\n",
      "I1125 19:55:39.891422 140283058251648 base_any2vec.py:1346] EPOCH - 5 : training on 79407 raw words (76987 effective words) took 0.2s, 417965 effective words/s\n",
      "I1125 19:55:39.892918 140283058251648 base_any2vec.py:1382] training on a 397035 raw words (384981 effective words) took 0.9s, 407656 effective words/s\n",
      "I1125 19:55:39.896760 140283058251648 utils.py:542] saving Word2Vec object under word2vec_model.model, separately None\n",
      "I1125 19:55:39.897893 140283058251648 utils.py:648] not storing attribute vectors_norm\n",
      "I1125 19:55:39.898634 140283058251648 utils.py:648] not storing attribute cum_table\n",
      "I1125 19:55:40.117016 140283058251648 utils.py:556] saved word2vec_model.model\n",
      "W1125 19:55:40.118380 140283058251648 base_any2vec.py:1182] Effective 'alpha' higher than previous training cycles\n",
      "I1125 19:55:40.119220 140283058251648 base_any2vec.py:1210] training model with 4 workers on 12844 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "I1125 19:55:40.326559 140283058251648 base_any2vec.py:349] worker thread finished; awaiting finish of 3 more threads\n",
      "I1125 19:55:40.328417 140283058251648 base_any2vec.py:349] worker thread finished; awaiting finish of 2 more threads\n",
      "I1125 19:55:40.330063 140283058251648 base_any2vec.py:349] worker thread finished; awaiting finish of 1 more threads\n",
      "I1125 19:55:40.331993 140283058251648 base_any2vec.py:349] worker thread finished; awaiting finish of 0 more threads\n",
      "I1125 19:55:40.333487 140283058251648 base_any2vec.py:1346] EPOCH - 1 : training on 79407 raw words (76985 effective words) took 0.2s, 393885 effective words/s\n",
      "W1125 19:55:40.334832 140283058251648 base_any2vec.py:1362] EPOCH - 1 : supplied raw word count (79407) did not equal expected count (12844)\n",
      "I1125 19:55:40.551728 140283058251648 base_any2vec.py:349] worker thread finished; awaiting finish of 3 more threads\n",
      "I1125 19:55:40.553533 140283058251648 base_any2vec.py:349] worker thread finished; awaiting finish of 2 more threads\n",
      "I1125 19:55:40.554623 140283058251648 base_any2vec.py:349] worker thread finished; awaiting finish of 1 more threads\n",
      "I1125 19:55:40.555606 140283058251648 base_any2vec.py:349] worker thread finished; awaiting finish of 0 more threads\n",
      "I1125 19:55:40.556778 140283058251648 base_any2vec.py:1346] EPOCH - 2 : training on 79407 raw words (76986 effective words) took 0.2s, 382068 effective words/s\n",
      "W1125 19:55:40.559532 140283058251648 base_any2vec.py:1362] EPOCH - 2 : supplied raw word count (79407) did not equal expected count (12844)\n",
      "I1125 19:55:40.754215 140283058251648 base_any2vec.py:349] worker thread finished; awaiting finish of 3 more threads\n",
      "I1125 19:55:40.756024 140283058251648 base_any2vec.py:349] worker thread finished; awaiting finish of 2 more threads\n",
      "I1125 19:55:40.757598 140283058251648 base_any2vec.py:349] worker thread finished; awaiting finish of 1 more threads\n",
      "I1125 19:55:40.759692 140283058251648 base_any2vec.py:349] worker thread finished; awaiting finish of 0 more threads\n",
      "I1125 19:55:40.760534 140283058251648 base_any2vec.py:1346] EPOCH - 3 : training on 79407 raw words (77013 effective words) took 0.2s, 440700 effective words/s\n",
      "W1125 19:55:40.761836 140283058251648 base_any2vec.py:1362] EPOCH - 3 : supplied raw word count (79407) did not equal expected count (12844)\n",
      "I1125 19:55:40.950509 140283058251648 base_any2vec.py:349] worker thread finished; awaiting finish of 3 more threads\n",
      "I1125 19:55:40.952134 140283058251648 base_any2vec.py:349] worker thread finished; awaiting finish of 2 more threads\n",
      "I1125 19:55:40.953269 140283058251648 base_any2vec.py:349] worker thread finished; awaiting finish of 1 more threads\n",
      "I1125 19:55:40.956656 140283058251648 base_any2vec.py:349] worker thread finished; awaiting finish of 0 more threads\n",
      "I1125 19:55:40.957492 140283058251648 base_any2vec.py:1346] EPOCH - 4 : training on 79407 raw words (76987 effective words) took 0.2s, 428490 effective words/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1125 19:55:40.958196 140283058251648 base_any2vec.py:1362] EPOCH - 4 : supplied raw word count (79407) did not equal expected count (12844)\n",
      "I1125 19:55:40.958857 140283058251648 base_any2vec.py:1382] training on a 317628 raw words (307971 effective words) took 0.8s, 367292 effective words/s\n",
      "W1125 19:55:40.959497 140283058251648 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(307971, 317628)"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert original corpus to a list of vectors\n",
    "\n",
    "word2vec_model = Word2Vec(corpus_docs, size=100, window=5, min_count=1, workers=4)\n",
    "word2vec_model.save(\"word2vec_model.model\")\n",
    "word2vec_model.train(corpus_docs, total_words=len(dictionary), epochs=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A nice feature of gensim is that we can input any list of words we like, and the trained model will infer a vector using the `model.infer_vector` function. (These words should be tokenized the same way as the original input.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-25T20:24:14.560562Z",
     "start_time": "2019-11-25T20:24:14.551287Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.8970243 ,  0.03396155,  0.03367773,  0.18531203,  0.23543327,\n",
       "       -0.2886431 ,  0.11652056,  0.33937278,  0.37848   , -0.35977128,\n",
       "        0.3452646 , -0.10515809,  0.11883105, -0.33633265, -0.02338399,\n",
       "        0.3911174 ,  0.30608284, -0.60811317, -0.14078681,  0.04302975,\n",
       "        0.17476422,  0.23118949,  0.07585598,  0.23363063, -0.07966822,\n",
       "       -0.29803884,  0.07596204,  0.09144077,  0.24587053, -0.01020437,\n",
       "        0.23842847, -0.28817824,  0.20111568, -0.00382623, -0.49379253,\n",
       "        0.04864242, -0.18418962,  0.13911545,  0.39039007, -0.24681053,\n",
       "       -0.2171303 , -0.15378155,  0.33905306,  0.22656122, -0.32394376,\n",
       "        0.2269503 ,  0.07785336,  0.17612188,  0.14768776, -0.13606453],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc2vec_model.infer_vector(\"president united states gave speech today.\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-25T22:53:28.030312Z",
     "start_time": "2019-11-25T22:53:28.019594Z"
    }
   },
   "outputs": [],
   "source": [
    "doc2vec_model.docvecs[0]\n",
    "# nb: Care must be taken with oov terms -- we need to make sure \n",
    "# to choose models and methods, e.g. Laplacian smoothing, that\n",
    "# can deal with values it's never seen before \n",
    "# there may be artifacts here of gensim version, see GitHub issues\n",
    "# to do: try updating gensim in separate venv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also have the options to instantiate word2vec models with or without ngrams to quickly assess whether they are useful and which option is more advantageous."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Traditional NLP methods use a largely context-agnostic method of computing similarity that nonetheless produces good results much of the time. TF-IDF improves on naive frequency counts by weighting rarer words more heavily and penalizing too-common words on a log scale. Still, its information gain is largely limited by the count of the words in the vocabulary.\n",
    "\n",
    "Word2vec consists of a shallow neural network that represents the terms in a corpus as weights as calculated by a model that relates them to one another within a given window. Within that window, it is context and direction agnostic: we cannot recover word order, i.e., how close or far the context word is from the center word. Moreover, it produces one embedding per vocabulary word, which can cause problems for polysemy, unlike BERT, which is bi-directionally context sensitive. Still, with other linguistic features (e.g., syntactic dependencies, entity extraction and coreference resolution, part-of-speech tagging, ngram modeling) we can increase the model's capacity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-25T19:58:08.967922Z",
     "start_time": "2019-11-25T19:58:04.795823Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1125 19:58:04.797906 140283058251648 phrases.py:475] collecting all words and their counts\n",
      "I1125 19:58:04.799263 140283058251648 phrases.py:482] PROGRESS: at sentence #0, processed 0 words and 0 word types\n",
      "I1125 19:58:04.980337 140283058251648 phrases.py:505] collected 59101 word types from a corpus of 79407 words (unigram + bigrams) and 6916 sentences\n",
      "I1125 19:58:04.981020 140283058251648 phrases.py:558] using 59101 counts as vocab in Phrases<0 vocab, min_count=5, threshold=10.0, max_vocab_size=40000000>\n",
      "I1125 19:58:04.982093 140283058251648 word2vec.py:1588] collecting all words and their counts\n",
      "I1125 19:58:04.983121 140283058251648 word2vec.py:1573] PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "I1125 19:58:05.451836 140283058251648 word2vec.py:1596] collected 13162 word types from a corpus of 75506 raw words and 6916 sentences\n",
      "I1125 19:58:05.452785 140283058251648 word2vec.py:1647] Loading a fresh vocabulary\n",
      "I1125 19:58:05.486203 140283058251648 word2vec.py:1671] effective_min_count=1 retains 13162 unique words (100% of original 13162, drops 0)\n",
      "I1125 19:58:05.487075 140283058251648 word2vec.py:1677] effective_min_count=1 leaves 75506 word corpus (100% of original 75506, drops 0)\n",
      "I1125 19:58:05.554697 140283058251648 word2vec.py:1736] deleting the raw counts dictionary of 13162 items\n",
      "I1125 19:58:05.555781 140283058251648 word2vec.py:1739] sample=0.001 downsamples 5 most-common words\n",
      "I1125 19:58:05.556515 140283058251648 word2vec.py:1742] downsampling leaves estimated 73478 word corpus (97.3% of prior 75506)\n",
      "I1125 19:58:05.597556 140283058251648 base_any2vec.py:1022] estimated required memory for 13162 words and 100 dimensions: 17110600 bytes\n",
      "I1125 19:58:05.598593 140283058251648 word2vec.py:1888] resetting layer weights\n",
      "I1125 19:58:05.759577 140283058251648 base_any2vec.py:1210] training model with 3 workers on 13162 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "I1125 19:58:06.382937 140283058251648 base_any2vec.py:349] worker thread finished; awaiting finish of 2 more threads\n",
      "I1125 19:58:06.384305 140283058251648 base_any2vec.py:349] worker thread finished; awaiting finish of 1 more threads\n",
      "I1125 19:58:06.393935 140283058251648 base_any2vec.py:349] worker thread finished; awaiting finish of 0 more threads\n",
      "I1125 19:58:06.395028 140283058251648 base_any2vec.py:1346] EPOCH - 1 : training on 75506 raw words (73492 effective words) took 0.6s, 116913 effective words/s\n",
      "I1125 19:58:07.027728 140283058251648 base_any2vec.py:349] worker thread finished; awaiting finish of 2 more threads\n",
      "I1125 19:58:07.029187 140283058251648 base_any2vec.py:349] worker thread finished; awaiting finish of 1 more threads\n",
      "I1125 19:58:07.039226 140283058251648 base_any2vec.py:349] worker thread finished; awaiting finish of 0 more threads\n",
      "I1125 19:58:07.040329 140283058251648 base_any2vec.py:1346] EPOCH - 2 : training on 75506 raw words (73458 effective words) took 0.6s, 115106 effective words/s\n",
      "I1125 19:58:07.643746 140283058251648 base_any2vec.py:349] worker thread finished; awaiting finish of 2 more threads\n",
      "I1125 19:58:07.644964 140283058251648 base_any2vec.py:349] worker thread finished; awaiting finish of 1 more threads\n",
      "I1125 19:58:07.653989 140283058251648 base_any2vec.py:349] worker thread finished; awaiting finish of 0 more threads\n",
      "I1125 19:58:07.654963 140283058251648 base_any2vec.py:1346] EPOCH - 3 : training on 75506 raw words (73514 effective words) took 0.6s, 121082 effective words/s\n",
      "I1125 19:58:08.276439 140283058251648 base_any2vec.py:349] worker thread finished; awaiting finish of 2 more threads\n",
      "I1125 19:58:08.278230 140283058251648 base_any2vec.py:349] worker thread finished; awaiting finish of 1 more threads\n",
      "I1125 19:58:08.287953 140283058251648 base_any2vec.py:349] worker thread finished; awaiting finish of 0 more threads\n",
      "I1125 19:58:08.288920 140283058251648 base_any2vec.py:1346] EPOCH - 4 : training on 75506 raw words (73520 effective words) took 0.6s, 117528 effective words/s\n",
      "I1125 19:58:08.946668 140283058251648 base_any2vec.py:349] worker thread finished; awaiting finish of 2 more threads\n",
      "I1125 19:58:08.949170 140283058251648 base_any2vec.py:349] worker thread finished; awaiting finish of 1 more threads\n",
      "I1125 19:58:08.962675 140283058251648 base_any2vec.py:349] worker thread finished; awaiting finish of 0 more threads\n",
      "I1125 19:58:08.963664 140283058251648 base_any2vec.py:1346] EPOCH - 5 : training on 75506 raw words (73510 effective words) took 0.7s, 112003 effective words/s\n",
      "I1125 19:58:08.964568 140283058251648 base_any2vec.py:1382] training on a 377530 raw words (367494 effective words) took 3.2s, 114685 effective words/s\n"
     ]
    }
   ],
   "source": [
    "# learn a word2vec model with multiword ngram expressions\n",
    "    \n",
    "bigram_transformer = Phrases(corpus_docs)\n",
    "phrase_model = Word2Vec(bigram_transformer[corpus_docs], min_count=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some good visualizations for this kind of data could be t-SNE, or simply a mapping of the highest contributing similarity features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.tensorboard.plugins import projector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-25T21:33:23.091500Z",
     "start_time": "2019-11-25T21:33:23.086358Z"
    }
   },
   "source": [
    "### PyTorch modeling and visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coda: Installing transformers in spaCy, generating Gold corpus for updating BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can improve our spaCy BERT model by creating a GoldParse corpus and updating the out-of-the-box transformer with our data. If the data is relatively simple, it's fairly simple to create a jsonlines file of training tuples and update the model from the command line or in a script: \n",
    "\n",
    "```sh\n",
    "!cat msr-para-train.tsv | jq --raw-input --slurp 'split(\"\\n\") | map(split(\"\\t\")) | .[0:-1] | map( { \"id_1\": .[1], \"doc_1\": .[3], \"label\": .[0], \"id_2\": .[2], \"doc_2\": .[4], \"id_2\": .[1] } )'\n",
    "```\n",
    "\n",
    "To install the spaCy compatible transformers, run:\n",
    "\n",
    "```sh\n",
    "$ !pip install spacy-transformers && python -m spacy download en_trf_bertbaseuncased_lg\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-24T15:56:45.659309Z",
     "start_time": "2019-11-24T15:56:45.652765Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "this is a document for testing"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp.(\"this is a document for testing\")\n",
    "nlp2.make_doc(\" \".join([token.lower_ for token in doc]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-24T16:37:44.045246Z",
     "start_time": "2019-11-24T16:37:44.041888Z"
    }
   },
   "outputs": [],
   "source": [
    "from spacy.util import minibatch\n",
    "textcat = nlp.create_pipe(\"trf_textcat\", config={\"exclusive_classes\": True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-24T17:14:15.994699Z",
     "start_time": "2019-11-24T17:14:15.959507Z"
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "[E007] 'trf_textcat' already exists in pipeline. Existing names: ['sentencizer', 'trf_wordpiecer', 'trf_tok2vec', 'trf_textcat']",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-8b419ddb1297>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"POSITIVE\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"NEGATIVE\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mtextcat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_label\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_pipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtextcat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipe_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/spacy/language.py\u001b[0m in \u001b[0;36madd_pipe\u001b[0;34m(self, component, name, before, after, first, last)\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_component_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomponent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipe_names\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 327\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE007\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipe_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    328\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbefore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mafter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfirst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlast\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE006\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: [E007] 'trf_textcat' already exists in pipeline. Existing names: ['sentencizer', 'trf_wordpiecer', 'trf_tok2vec', 'trf_textcat']"
     ]
    }
   ],
   "source": [
    "for label in (\"POSITIVE\", \"NEGATIVE\"):\n",
    "    textcat.add_label(label)\n",
    "nlp.add_pipe(textcat)\n",
    "print(nlp.pipe_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example training corpus illustration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-24T23:00:22.030500Z",
     "start_time": "2019-11-24T23:00:22.026531Z"
    }
   },
   "outputs": [],
   "source": [
    "TRAIN_DATA = [\n",
    "    (\n",
    "        \"We trade in over the counter corporate bonds in Europe and Asia.\",\n",
    "        {\n",
    "            \"label\": \"POSITIVE\",\n",
    "            \"id\": 1\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        \"I like sushi and sashimi, but not nigiri.\",\n",
    "        {\n",
    "            \"label\": \"NEGATIVE\",\n",
    "            \"id\": 2\n",
    "           \n",
    "        },\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-24T23:00:34.101562Z",
     "start_time": "2019-11-24T23:00:29.102521Z"
    }
   },
   "outputs": [],
   "source": [
    "optimizer = nlp.resume_training()\n",
    "for i in range(10):\n",
    "    random.shuffle(TRAIN_DATA)\n",
    "    losses = {}\n",
    "    for batch in minibatch(TRAIN_DATA, size=8):\n",
    "        texts, cats = zip(*batch)\n",
    "        nlp.update(texts, cats, sgd=optimizer, losses=losses)\n",
    "    print(i, losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SpaCy requires _tuples_ of jsonlines attributes and strings, not simply jsonlines, so depending on how we process our text, we may need to write our `jq` or python pipeline differently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-24T17:50:12.149975Z",
     "start_time": "2019-11-24T17:50:12.135876Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Amrozi accused his brother, whom he called \"the witness\", of deliberately distorting his evidence., {'label': 1, 'id': 702876}),\n",
      "(Referring to him as only \"the witness\", Amrozi accused his brother of deliberately distorting his evidence., {'label': 1, 'id': 702977}),\n",
      "(Yucaipa owned Dominick's before selling the chain to Safeway in 1998 for $2.5 billion., {'label': 1, 'id': 2108705}),\n",
      "(Yucaipa bought Dominick's in 1995 for $693 million and sold it to Safeway for $1.8 billion in 1998., {'label': 1, 'id': 2108831}),\n",
      "(They had published an advertisement on the Internet on June 10, offering the cargo for sale, he added., {'label': 1, 'id': 1330381}),\n",
      "(On June 10, the ship's owners had published an advertisement on the Internet, offering the explosives for sale., {'label': 1, 'id': 1330521}),\n",
      "(Around 0335 GMT, Tab shares were up 19 cents, or 4.4%, at A$4.56, having earlier set a record high of A$4.57., {'label': 1, 'id': 3344667}),\n",
      "(Tab shares jumped 20 cents, or 4.6%, to set a record closing high at A$4.57., {'label': 1, 'id': 3344648}),\n",
      "(The stock rose $2.11, or about 11 percent, to close Friday at $21.51 on the New York Stock Exchange., {'label': 1, 'id': 1236820}),\n",
      "(PG&E Corp. shares jumped $1.63 or 8 percent to $21.03 on the New York Stock Exchange on Friday., {'label': 1, 'id': 1236712}),\n",
      "(Revenue in the first quarter of the year dropped 15 percent from the same period a year earlier., {'label': 1, 'id': 738533}),\n",
      "(With the scandal hanging over Stewart's company, revenue the first quarter of the year dropped 15 percent from the same period a year earlier., {'label': 1, 'id': 737951}),\n",
      "(The Nasdaq had a weekly gain of 17.27, or 1.2 percent, closing at 1,520.15 on Friday., {'label': 1, 'id': 264589}),\n",
      "(The tech-laced Nasdaq Composite .IXIC rallied 30.46 points, or 2.04 percent, to 1,520.15., {'label': 1, 'id': 264502}),\n",
      "(The DVD-CCA then appealed to the state Supreme Court., {'label': 1, 'id': 579975}),\n",
      "(The DVD CCA appealed that decision to the U.S. Supreme Court., {'label': 1, 'id': 579810}),\n",
      "(That compared with $35.18 million, or 24 cents per share, in the year-ago period., {'label': 1, 'id': 3114205}),\n",
      "(Earnings were affected by a non-recurring $8 million tax benefit in the year-ago period., {'label': 1, 'id': 3114194}),\n",
      "(He said the foodservice pie business doesn't fit the company's long-term growth strategy., {'label': 1, 'id': 1355540}),\n",
      "(The foodservice pie business does not fit our long-term growth strategy.\r\n",
      "0\t222621\t222514\tShares of Genentech, a much larger company with several products on the market, rose more than 2 percent.\tShares of Xoma fell 16 percent in early trade, while shares of Genentech, a much larger company with several products on the market, were up 2 percent.\r\n",
      "0\t3131772\t3131625\tLegislation making it harder for consumers to erase their debts in bankruptcy court won overwhelming House approval in March.\tLegislation making it harder for consumers to erase their debts in bankruptcy court won speedy, House approval in March and was endorsed by the White House.\r\n",
      "0\t58747\t58516\tThe Nasdaq composite index increased 10.73, or 0.7 percent, to 1,514.77.\tThe Nasdaq Composite index, full of technology stocks, was lately up around 18 points.\r\n",
      "1\t1464126\t1464107\tBut he added group performance would improve in the second half of the year and beyond.\tDe Sole said in the results statement that group performance would improve in the second half of the year and beyond.\r\n",
      "1\t771416\t771467\tHe told The Sun newspaper that Mr. Hussein's daughters had British schools and hospitals in mind when they decided to ask for asylum.\tSaddam's daughters had British schools and hospitals in mind when they decided to ask for asylum -- especially the schools,\" he told The Sun., {'label': 1, 'id': 1355592}),\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    doc1 = text[\"doc_1\"].iloc[i]\n",
    "    id_1 = text[\"id_1\"].iloc[i]\n",
    "    label_1 = text[\"label_1\"].iloc[i]\n",
    "    doc2 = text[\"doc_2\"].iloc[i]\n",
    "    id_2 = text[\"id_2\"].iloc[i]\n",
    "    label_2 = text[\"label_2\"].iloc[i]\n",
    "    line1 = encode_line(doc1, id_1, label_1)\n",
    "    line2 = encode_line(doc2, id_2, label_2)\n",
    "    print(line1)\n",
    "    print(line2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
